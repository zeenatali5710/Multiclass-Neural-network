{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\tMulticlass neural network classifier\n",
    "### a)\tImplementation and convergence criterion:\n",
    "The multiclass neural network classifier with one hidden layer is implemented using python. The implementation has two parts – <br> \n",
    "*(1) a class named MulticlassNN.py is implemented that contains the definition of training model and* <br>\n",
    "*(2) a train-test implementation named TrainTestNN.ipynb that imports training model MulticlassNN.py to run the training and test data (dev data) with different configurations.* <br><br>\n",
    "**In MulticlassNN Neural network model**,\n",
    "- no. of layers (input, hidden and output), no. of neurons per hidden layer, activation function and cost function are initialized. <br>\n",
    "- An input layer to hidden layer and a hidden layer amongst themselves are connected using activation functions which implies, output of layer is input to the next layer after activation function is acted upon each set of inputs until it reaches final output layer. I have used three activation functions – sigmoid, tanh and softmax, implemented using NumPy package’s inbuilt functions – tanh, exp, sum and divide. <br>\n",
    "- The training model has three steps, viz. forward pass, error computation and backward pass. Each instance will go through activation function while forward pass and error is computed at output. Based on error, it will be back propagated by taking the derivative deltas w.r.t activation and multiplying it with learning rate to decide on size of learning step, i.e., calculating the gradient descent.\n",
    "- Error is computed based on two cost functions, viz. cross entropy or mean square error. <br>\n",
    "- Accuracy percentage is then computed based on total correctly classified predicted labels divided by total labels in the input data multiplied by 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created By: Ali, Zeenat\n",
    "# Homework 4: CMSC 678 Spring 2020\n",
    "import numpy as np\n",
    "import dill as dill\n",
    "\n",
    "class MulticlassNN:\n",
    "    # Initialize no of layers( input, hidden and output), no. of neurons, activation and cost functions\n",
    "    def __init__(self, noOfLayers, neurons, activation, costFunction):\n",
    "        self.layers = []\n",
    "        self.noOfLayers = noOfLayers\n",
    "        self.neurons = neurons\n",
    "        self.cost_function = costFunction\n",
    "\n",
    "        # Declare vector size for each neuron in each layer, the elements in neuron list should match no. of layers\n",
    "        if not noOfLayers == len(neurons):\n",
    "            raise ValueError(\"Layers and neuron count mismatch\")\n",
    "\n",
    "        # Include next layer neurons for input layer and all hidden layers (excluding output layer)\n",
    "        for x in range(noOfLayers):\n",
    "            if x != noOfLayers-1:\n",
    "                layer_x = layer(neurons[x], neurons[x+1], activation[x])\n",
    "            else:\n",
    "                layer_x = layer(neurons[x], 0, activation[x])\n",
    "            self.layers.append(layer_x)\n",
    "\n",
    "    # Each instance will go through activation function while forward pass, then error is computed at output;\n",
    "    # based on error, it will be back propagated, weights are updated and again passed forward until the classifier\n",
    "    # converges and a decent accuracy is attained.\n",
    "    def trainNeuralNetwork(self, batch, trainingDataX, trainingLabelY, epochs, learningRate, filename):\n",
    "        self.batch = batch\n",
    "        self.learningRate = learningRate\n",
    "        for j in range(epochs):\n",
    "            i = 0\n",
    "            print(\"***** EPOCH#: \", j+1, \"out of\", epochs, \" *******\")\n",
    "            while i+batch != len(trainingDataX):\n",
    "                print(\"Training with \", i+batch+1, \"/\", len(trainingDataX), end=\"\\r\")\n",
    "                self.error = 0\n",
    "                self.forwardPass(trainingDataX[i:i+batch])\n",
    "                self.computeError(trainingLabelY[i:i+batch])\n",
    "                self.backPropagate(trainingLabelY[i:i+batch])\n",
    "                i += batch\n",
    "            self.error /= batch\n",
    "            print(\"***** \\nError : \", self.error, \"*****\")\n",
    "        dill.dump_session(filename)\n",
    "\n",
    "    # Each forward pass will update weights based on an activation function\n",
    "    def forwardPass(self, TrainingDataX):\n",
    "        self.layers[0].activations = TrainingDataX\n",
    "        for i in range(self.noOfLayers-1):\n",
    "            tempMat = np.add(np.matmul(self.layers[i].activations, self.layers[i].currentLayerWeights), self.layers[i].currentLayerBias)\n",
    "            if self.layers[i+1].activation == \"sigmoid\":\n",
    "                self.layers[i+1].activations = self.sigmoid(tempMat)\n",
    "            elif self.layers[i+1].activation == \"tanh\":\n",
    "                self.layers[i+1].activations = self.tanh(tempMat)\n",
    "            elif self.layers[i+1].activation == \"softmax\":\n",
    "                self.layers[i+1].activations = self.softmax(tempMat)\n",
    "            else:\n",
    "                self.layers[i+1].activations = tempMat\n",
    "\n",
    "    # Activation function = sigmoid\n",
    "    def sigmoid(self, layer):\n",
    "        return np.divide(1, np.add(1, np.exp(np.negative(layer))))\n",
    "\n",
    "    # Activation function = tanh\n",
    "    def tanh(self, layer):\n",
    "        return np.tanh(layer)\n",
    "    \n",
    "    # Activation function = softmax\n",
    "    def softmax(self, layer):\n",
    "        exp = np.exp(layer)\n",
    "        if isinstance(layer[0], np.ndarray):\n",
    "            return exp/np.sum(exp, axis=1, keepdims=True)\n",
    "        else:\n",
    "            return exp/np.sum(exp, keepdims=True)\n",
    "\n",
    "    # Error can be calculate based on cost function as cross entropy or mean squared\n",
    "    def computeError(self, trainingLabelY):\n",
    "        if len(trainingLabelY[0]) != self.layers[self.noOfLayers-1].currentLayerNeurons:\n",
    "            print (\"Error: Label Y and output layer matrix dimension mismatch.\")\n",
    "            return\n",
    "        if self.cost_function == \"meanSquared\":\n",
    "            self.error += np.mean(np.divide(np.square(np.subtract(trainingLabelY, self.layers[self.noOfLayers-1].activations)), 2))\n",
    "        elif self.cost_function == \"crossEntropy\":\n",
    "            self.error += np.negative(np.sum(np.multiply(trainingLabelY, np.log(self.layers[self.noOfLayers-1].activations))))\n",
    "\n",
    "    # Once we have error, we apply back propagation by differeniating w.r.t activation and update weights to improve accuracy \n",
    "    def backPropagate(self, trainingLabelY):\n",
    "        targets = trainingLabelY\n",
    "        i = self.noOfLayers-1\n",
    "        y = self.layers[i].activations\n",
    "        deltab = np.multiply(y, np.multiply(1-y, targets-y))\n",
    "        deltaw = np.matmul(np.asarray(self.layers[i-1].activations).T, deltab)\n",
    "        new_weights = self.layers[i-1].currentLayerWeights - self.learningRate * deltaw\n",
    "        new_bias = self.layers[i-1].currentLayerBias - self.learningRate * deltab\n",
    "        for i in range(i-1, 0, -1):\n",
    "            y = self.layers[i].activations\n",
    "            deltab = np.multiply(y, np.multiply(1-y, np.sum(np.multiply(new_bias, self.layers[i].currentLayerBias)).T))\n",
    "            deltaw = np.matmul(np.asarray(self.layers[i-1].activations).T, np.multiply(y, np.multiply(1-y, np.sum(np.multiply(new_weights, self.layers[i].currentLayerWeights),axis=1).T)))\n",
    "            self.layers[i].currentLayerWeights = new_weights\n",
    "            self.layers[i].currentLayerBias = new_bias\n",
    "            new_weights = self.layers[i-1].currentLayerWeights - self.learningRate * deltaw\n",
    "            new_bias = self.layers[i-1].currentLayerBias - self.learningRate * deltab\n",
    "        self.layers[0].currentLayerWeights = new_weights\n",
    "        self.layers[0].currentLayerBias = new_bias\n",
    "\n",
    "    \n",
    "    def computeAccuracy(self, filename, inputDataX, labelY):\n",
    "        dill.load_session(filename)\n",
    "        self.batch = len(inputDataX)\n",
    "        self.forwardPass(inputDataX)\n",
    "        a = self.layers[self.noOfLayers-1].activations\n",
    "        a[np.where(a==np.max(a))] = 1\n",
    "        a[np.where(a!=np.max(a))] = 0\n",
    "        total=0\n",
    "        correct=0\n",
    "        for i in range(len(a)):\n",
    "            total += 1\n",
    "            if np.equal(a[i], labelY[i]).all():\n",
    "                correct += 1\n",
    "        print(\"Accuracy percentage: \", correct*100/total)\n",
    "        print(correct)\n",
    "        print(total)\n",
    "\n",
    "class layer:\n",
    "    def __init__(self, currentLayerNeurons, nextLayerNeurons, activation):\n",
    "        self.currentLayerNeurons = currentLayerNeurons\n",
    "        self.activation = activation\n",
    "        self.activations = np.zeros([currentLayerNeurons,1])\n",
    "        #Random distribution of weights for hidden layers and adding bias element\n",
    "        if nextLayerNeurons != 0:\n",
    "            self.currentLayerWeights = np.random.normal(0, 0.001, size=(currentLayerNeurons, nextLayerNeurons))\n",
    "            self.currentLayerBias = np.random.normal(0, 0.001, size=(1, nextLayerNeurons))\n",
    "        else:\n",
    "            self.currentLayerWeights = None\n",
    "            self.currentLayerBias = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In train-test implementation,**<br>\n",
    "- mnist data and MulticlassNN is imported. 60,000 samples are used as training data and 10,000 samples as test data or dev data.\n",
    "- The data is then converted to numpy array format to perform numpy operations.\n",
    "- A network is created using MulticlassNN neural network train model by plugging in appropriate configuration. The baseline model takes 3 layers, viz. input layer, one hidden layer and output layer. The input vector has 784 elements, hidden layer has 20 neurons and output layer has 10 elements corresponding to output class labels 0 to 9.\n",
    "- The activation for input layer is set as none as no activation is needed at input layer. I have taken tanh as the activation function for hidden layer and softmax activation function for output layer.\n",
    "- The error is calculated based on cost function as cross Entropy.\n",
    "- Once the confirgurations are set, training data is passed to configured network to train the model with 5 iterations (epochs) and a learning rate of 0.001. The training is done collectively in a single batch. A filename 'Result.pkl is used for pickling, i.e, to store the intermediary result to perform numpy operations.\n",
    "- Once the data is trained, the accuracy is calculated for the training data.\n",
    "- The trained model is then applied to test data for its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-mnist in c:\\softwares\\lib\\site-packages (0.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting import_ipynb\n",
      "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
      "Building wheels for collected packages: import-ipynb\n",
      "  Building wheel for import-ipynb (setup.py): started\n",
      "  Building wheel for import-ipynb (setup.py): finished with status 'done'\n",
      "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp37-none-any.whl size=2982 sha256=4e412a84f8e5b66eaf8da3fda86bf2fd5988996a988265bfdd8218de20f088bb\n",
      "  Stored in directory: C:\\Users\\zeena\\AppData\\Local\\pip\\Cache\\wheels\\b4\\7b\\e9\\a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
      "Successfully built import-ipynb\n",
      "Installing collected packages: import-ipynb\n",
      "Successfully installed import-ipynb-0.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/import-ipynb/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-mnist in c:\\softwares\\lib\\site-packages (0.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "***** EPOCH#:  1 out of 1  *******\n",
      "Training with  33276 / 6000004 / 60000 60000 1018 / 6000060000 / 60000 2334 / 60000 2568 / 60000 / 60000 60000/ 60000 3676 / 60000 4311 / 600004794 / 600005028 / 60000 / 60000 7061 / 60000 7286 / 60000/ 60000 7721 / 6000060000 8171 / 60000 8388 / 600006000060000/ 60000 10014 / 60000 10248 / 60000 10444 / 600006000011771 / 60000 60000 / 6000012817 / 6000060000 13475 / 60000 13913 / 60000 14545 / 6000060000 60000/ 60000 15336 / 60000 15785 / 60000 16004 / 60000 / 6000017640 / 60000 17840 / 60000 60000 18835 / 60000 19185 / 60000 / 60000 20332 / 60000/ 60000 21339 / 60000 21939 / 60000 22129 / 6000060000 23106 / 60000 23310 / 60000 23520 / 60000 23731 / 60000 / 60000 24605 / 6000060000 60000600006000025765 / 60000 26166 / 60000 60000 27783 / 60000 28024 / 6000060000/ 6000029957 / 60000 30164 / 60000 / 60000/ 600006000031231 / 60000 31440 / 60000 31885 / 60000 60000 32912 / 60000"
     ]
    }
   ],
   "source": [
    "%pip install python-mnist\n",
    "%pip install import_ipynb\n",
    "\n",
    "import numpy as np\n",
    "import MulticlassNW as nn\n",
    "from mnist import MNIST\n",
    "\n",
    "#Import MNIST data and load training and Dev/Test data \n",
    "mnist = MNIST('mnist-dataset')\n",
    "TrainingDataX, TrainingLabelY = mnist.load_training() # 60000 training samples\n",
    "TestDataX, TestLabelY = mnist.load_testing()          # 10000 test samples\n",
    "\n",
    "#Converting data to numpy array format\n",
    "TrainingDataX = np.asarray(TrainingDataX).astype(np.float32)\n",
    "TrainingLabelY = np.asarray(TrainingLabelY).astype(np.int32)\n",
    "TestDataX = np.asarray(TestDataX).astype(np.float32)\n",
    "TestLabelY = np.asarray(TestLabelY).astype(np.int32)\n",
    "\n",
    "# Baseline: Train Neural Network - Single hidden layer using sigmoid activation and output layer using softmax activation\n",
    "NumberOfLabels = 10\n",
    "Trainingclass = np.eye(NumberOfLabels)[TrainingLabelY] # all zeroes except ones on diagonals \n",
    "Network = nn.MulticlassNN(3, [784, 100, 10], [None, \"tanh\", \"softmax\"], costFunction=\"crossEntropy\")\n",
    "Network.trainNeuralNetwork(1, trainingDataX=TrainingDataX, trainingLabelY=Trainingclass, epochs=1, learningRate=0.1, filename=\"Result.pkl\")\n",
    "print(\"Training Accuracy\")\n",
    "Network.computeAccuracy(\"Result.pkl\", TrainingDataX, Trainingclass)\n",
    "\n",
    "# Test Baseline Neural Network\n",
    "TestClass = np.eye(NumberOfLabels)[TestLabelY]\n",
    "print(\"Testing Accuracy\")\n",
    "Network.computeAccuracy(\"Result.pkl\", TestDataX, TestClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convergence criteria:**<br>\n",
    "- Weights and bias have been initialized based on random normal distribution.\n",
    "- For each derivative w.r.t activation while back propagating, weights are updated by subtracting the gradients from current weight and again passed forward to check for error at each neuron. <br>\n",
    "- The process continues until the error stabilizes and stops reducing further based on cost function.\n",
    "- Eventually the updated weights starts giving right classification of labels and converges to an accuracy of about 87%.\n",
    "\n",
    "### b)\tValidation using XOR input data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Training Accuracy/ 8 3 / 8888 7 / 8 3 / 84 / 88/ 88 / 8/ 8 88 4 / 8/ 8/ 8/ 8 8 / 8 4 / 88 5 / 8 / 8 8 4 / 8 / 8/ 84 / 8 8 8/ 88 6 / 8 7 / 88 / 8 8 / 8 3 / 8 2 / 86 / 8 8/ 8 8 / 8 8 8 4 / 8 6 / 8/ 8 / 8/ 8 8 / 8/ 8 / 88 8 / 8 / 888/ 88 8 2 / 8 4 / 8 8 / 8 6 / 8 7 / 8 8/ 8 / 87 / 8 2 / 8 8 / 8 6 / 8 / 8/ 83 / 8 8 4 / 88 / 888 88 / 8 7 / 88 8 3 / 8 3 / 8 8 / 8 6 / 8 6 / 8 6 / 886 / 8 8 / 8 / 888 / 8 2 / 8/ 8 6 / 8/ 8/ 88 8 3 / 8/ 8 2 / 8 / 8 3 / 8 8 / 8 8 / 88 / 8 8 8 / 8 / 8 / 8 4 / 8 8888 / 8 / 8/ 8 8 8 / 8/ 8 2 / 88 / 8 6 / 8 6 / 8 6 / 8 / 8885 / 8 7 / 83 / 8 / 83 / 88 8 5 / 8 5 / 8 6 / 8 6 / 8 5 / 8 8 88 6 / 8 6 / 8 2 / 8 8 7 / 8/ 85 / 8 3 / 8 8 / 8 4 / 8 5 / 8 8 / 8 7 / 8 / 88 3 / 8 / 8 2 / 87 / 8 / 8 5 / 8 6 / 8 4 / 8 4 / 8 / 8 / 8/ 8 8/ 8 4 / 8 8 / 8 5 / 8 8 2 / 8/ 88 8 / 8/ 85 / 8 7 / 87 / 8/ 8 6 / 8 / 8 4 / 8 2 / 8 / 8 7 / 8 5 / 8 6 / 8 6 / 8 2 / 8/ 83 / 88 / 8 4 / 8 8 / 88 4 / 8/ 88 4 / 8 8 3 / 8 / 8 2 / 888 5 / 87 / 8 / 8 83 / 8 2 / 8 888 8 / 8 / 8 / 8 / 8/ 8 / 88/ 8 888/ 8 / 83 / 87 / 88 7 / 8 / 8/ 8 3 / 88 4 / 8 88 5 / 8 8 / 8 / 8 / 86 / 87 / 88 7 / 88 / 88 / 8 / 8 6 / 8 8 8 / 8/ 8/ 8/ 8/ 8 / 8 6 / 8 7 / 8 8 7 / 8/ 8 / 8 / 8 / 8 5 / 82 / 8 5 / 8 8 / 8 / 8 / 86 / 8 / 8 / 8 / 88 5 / 8 8 2 / 8 888 / 88 8 88 / 8 4 / 8 8 / 88 5 / 88 7 / 8/ 8/ 8 8 / 8 / 8/ 88 5 / 8 8 / 8 8 88 / 88 / 8 / 88 8 / 8 8 8 / 8 6 / 8 88 / 8 3 / 8 / 8 4 / 8 8 / 8 / 8 8 / 88 2 / 8/ 8 3 / 88 / 8 8/ 8 2 / 8 8 5 / 8 86 / 8 83 / 88 / 8 3 / 8 5 / 8 4 / 8 88 / 82 / 88/ 8 8 / 88 2 / 8 / 8/ 88 4 / 83 / 8 6 / 887 / 8 / 8 8 8 4 / 88 8 / 8 / 8 4 / 88 / 8 / 8 6 / 8 / 8 8 6 / 8/ 8 / 8 5 / 8 / 8/ 8 8 / 8/ 8/ 8/ 8 / 88 5 / 8/ 8 / 8 / 8 / 8 8 8/ 8 / 8 / 8 / 8 2 / 88 8 / 8 8 3 / 8 / 8 8 / 8 6 / 88 5 / 8/ 8 / 8/ 8 8 / 88/ 8 6 / 8 / 88/ 88 / 87 / 8/ 8 3 / 8/ 8 4 / 88 6 / 8 / 83 / 8 6 / 8 8 4 / 8 7 / 8 4 / 88 4 / 8 8/ 8 8/ 8 8 / 8 / 8 / 83 / 8 4 / 8 4 / 88 3 / 8 / 8 / 8 7 / 8 8 7 / 8 5 / 8 6 / 8 / 88/ 8 8 4 / 8 8 88 / 85 / 88 5 / 88 7 / 8 2 / 8 8 87 / 8 86 / 8 / 8 6 / 8 8 / 8 5 / 8/ 8 8 6 / 88 / 8/ 8/ 8 5 / 83 / 8 5 / 8 / 8/ 8 3 / 8 7 / 88 7 / 8 6 / 888 3 / 8\n",
      "Accuracy percentage:  37.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import MulticlassNN as nn\n",
    "from mnist import MNIST\n",
    "\n",
    "#Import  data and load training and Dev/Test data \n",
    "datum_1 = [0, 1, 1]\n",
    "datum_2 = [0, 1, 0]\n",
    "datum_3 = [0, 0, 1]\n",
    "datum_4 = [0, 0, 0]\n",
    "datum_5 = [1, 0, 1]\n",
    "datum_6 = [1, 1, 1]\n",
    "datum_7 = [1, 1, 0]\n",
    "datum_8 = [1, 0, 0]\n",
    "\n",
    "training_dataX = [datum_1, datum_2, datum_3, datum_4, datum_5, datum_6, datum_7, datum_8]\n",
    "training_labelY = [0, 1, 1, 0, 0, 1, 0, 1]\n",
    "\n",
    "# #Converting data to numpy array format\n",
    "TrainingDataX = np.array(training_dataX).astype(np.float32)\n",
    "TrainingLabelY = np.asarray(training_labelY).astype(np.int32)\n",
    "\n",
    "# Baseline: XOR data Train Neural Network - Single hidden layer using sigmoid activation and output layer using softmax activation\n",
    "NumberOfLabels = 2\n",
    "Trainingclass = np.eye(NumberOfLabels)[TrainingLabelY] # all zeroes except ones on diagonals \n",
    "print(Trainingclass)\n",
    "Network = nn.MulticlassNN(3, [3, 4, 2], [None, \"tanh\", \"softmax\"], costFunction=\"crossEntropy\")\n",
    "Network.trainNeuralNetwork(1, trainingDataX=TrainingDataX, trainingLabelY=Trainingclass, epochs=20000, learningRate=0.2, filename=\"Result.pkl\")\n",
    "print(\"Training Accuracy\")\n",
    "Network.computeAccuracy(\"Result.pkl\", TrainingDataX, TrainingLabelY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
